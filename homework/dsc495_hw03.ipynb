{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LOJWMGkV9tn6"
   },
   "source": [
    "# Assignment \\#03\n",
    "\n",
    "For this exercise, we're going to be using the college scorecard data. [The website for the data is here.](https://collegescorecard.ed.gov/data/)\n",
    "\n",
    "The database documentation can be found in this [file.](https://collegescorecard.ed.gov/assets/CollegeScorecardDataDictionary.xlsx) You'll need to refer to it through the assignment. \n",
    "\n",
    "**Scenario:** We have a big data problem where our data are spread across many different files. Although these files are not _that_ big, let's pretend that if they were one big file then that file wouldn't fit in memory (RAM). If we process each file individually, we can store in memory only the data we want to further examine. We want to extract some data from each file, do a calculation, and save only the results we are interested in.\n",
    "\n",
    "**Your Goal** is to cycle through data for public and private non-profit institutions located in Raleigh, North Carolina for all years of the college scorecard to calculate the cost of attendance through time. These data are split over individual CSV files for each school year. \n",
    "\n",
    "**Outcomes** \n",
    "1. Compute summary statistics across different files without merging them. \n",
    "\n",
    "\n",
    "**Credit Blocks**\n",
    "\n",
    "You must complete section 0. Then you may choose either section 1, section 2, or section 3\n",
    "\n",
    "0. Section 0 *reading only relevant information*\n",
    "1. Section 1 *computing the mean*\n",
    "1. Section 2 *computing the variance*\n",
    "1. Section 3 *computing the median*\n",
    "\n",
    "**Data Set**\n",
    "The data set is located in the shared folder explored during class at `/shared/dsc495/CollegeData`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## **Section 0**\n",
    "\n",
    "This will have two parts. The first is finding relevant files using string methods and list comprehension. The second is reading in only relevant data from those files. \n",
    "\n",
    "String objects have anumber of usefull built-in [methods](https://docs.python.org/2.5/lib/string-methods.html). Here, we consider the method `.find()`. \n",
    "\n",
    "Through properly named files, we know that the files of interest are those beginning with \"MERGED\". The above method returns the index where \"MERGED\" starts if found and -1 if not found. From this we can create a Boolean expression for whether a file is relevant or not. \n",
    "\n",
    "### Q1: Do the following:\n",
    "\n",
    "1. Create a boolean expression that resolves as True if \"MERGED\" is contained in a string and False if it is not contained in the string. \n",
    "1. Get a list of files from the directory `/shared/dsc495/CollegeData`. \n",
    "1. Using list comprehension and the boolean created above, get a list of the \"MERGED\" files. Store as `merge_files` and print the result. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_files = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    " read only relevant columns to our problem. Consider that this requires some level of knowledge about the data set. We know from the metadata that the columns related to cost are\n",
    " \n",
    " <div><center>\n",
    "    [\"TUITIONFEE_IN\", \"BOOKSUPPLY\", \"ROOMBOARD_ON\", \"OTHEREXPENSE_ON\"]\n",
    " </div>\n",
    "    \n",
    "We can use the same function as before, [`read_csv`](https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html), to read in data. However, now we will use the keyword arguments to set indices and to limit which columns we read in. \n",
    "    \n",
    "These options are `usecols` and `index_col`. We pass the list of names or indices of the columns we want to keep to `usecols`. And, we pass the name of the column to `index_col` to set it as the index. \n",
    "    \n",
    "Here the variable name for the instiutions is `INSTNM`. \n",
    "\n",
    "### Q2: For just the first file in the list, read in only the columns related to fees and set the index to the institution name. Store this variable as df. \n",
    "\n",
    "Hint: This can be done in a single line of code. \n",
    "\n",
    "Hint: Use `low_memory=FALSE` option to avoid type warnings. \n",
    "\n",
    "Hint: The subset to usecols is applied prior to setting the index column. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "One thing to note is that some of the values are not a number. We will assume for now that these missing values values are 0. To fill these values, we can use the `.fillna()` method. This will replace any missing values with the number given. Further we will use the `inplace=TRUE` option. This avoids copying the data frame to a new value. \n",
    "\n",
    "### Q3: To see this, do the following in order:\n",
    "   1. Print the first 5 rows of the `df`. \n",
    "   2. Use the `.fillna()` to replace NaNs with `0`, with the inplace option as True.\n",
    "   3. Print the first 5 rows of the `df`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "\n",
    "## Section 1\n",
    "\n",
    "Here, we want to compute the mean of the cost of attendance. The cost of attendance is the sum of the four columns given in section 0. \n",
    "\n",
    "Recall that the mean of a group of number is given as \n",
    "$$\\frac{1}{N} \\sum_{i=1}^N x_i. $$\n",
    "\n",
    "However, in order to compute this for data over multiple data sets, we must make some modifications. \n",
    "\n",
    "Complete the following portions to find the sum of these figures. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1: Using a for loop, read in each of the MERGED files, store the shape and the total sum in two lists. \n",
    "\n",
    "This may take approximately 15-20 seconds. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "shapes = []\n",
    "sums = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*** \n",
    "\n",
    "Alternatively, when given two groups, the mean can be written as \n",
    "\n",
    "$$\\frac{1}{N_1 + N_2} \\bigg( \\sum_{i=1}^{N_1} x_i + \\sum_{i=1}^{N_2} x_i  \\bigg) $$\n",
    "\n",
    "Here, the list `shapes` should contains the items `(rows, columns)` for each file, and `sums` should contain the sum of the four attendance columns sums. \n",
    "\n",
    "### Q2: Using an extension to the above formula, find the total cost of attendance of all institutions across all the files. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q3: Find the total number of observations across all files (i.e. total number of rows of all files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q4: Find the average cost of attendance for any institution in the years considered using your results from Q2 and Q3. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "## **Section 2**\n",
    "\n",
    "To estimate the variance, we use the below formula: \n",
    "$$\\frac{1}{N-1} \\sum_{i=1}^N (x_i - \\bar{x})^2. $$\n",
    "where $\\bar{x}$ is the mean as in Section 1. \n",
    "\n",
    "In order to find the variance, we will have to first find the mean of all costs of attendance, then pass through the data again. \n",
    "\n",
    "You may use $\\bar{x} = 8816$, or use the result from Section 1. \n",
    "\n",
    "### Q1. Why must we pass through the data twice?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cannot compute xbar and the summand every time. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Q2. Create two lists, one of the data frame shapes, one of the sum of $(x_i - \\bar{x})^2$ for each data frame. \n",
    "\n",
    "Hint: Attempt this for 1 data frame before using a loop over all files. \n",
    "Hint: the `.sum()` method has the option `axis=`. Try using the values 0 and 1 to see what sums are found. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "xbar = 8816\n",
    "shapes = []\n",
    "sums = []\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Q3. Use the lists `shapes` and `sums` from Q2 and find the variance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "## **Section 2**\n",
    "\n",
    "Here, we will compute the median. This is much more challenging given that we will have to continually loop through the data sets to find the median. \n",
    "\n",
    "The total number of observations is even (170026), so the median should split half the observations above and half below. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1: Get the median of the first file. And how many observations are above or below. \n",
    "\n",
    "Hint: use the `.sum()` and `.median()` methods. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*** \n",
    "\n",
    "A good initial value is the median of the first data set. \n",
    "\n",
    "### Q2: Loop over the files and find the total number above and below the initial medium guess. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_above = 0\n",
    "n_below = 0\n",
    "med_guess = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0\n"
     ]
    }
   ],
   "source": [
    "print(n_below, n_above)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "If we have more numbers above, we should increase the median guess. If we have more numbers below, we need to lower the median guess. \n",
    "\n",
    "You may notice that there is a seemingly large imbalance numbers above and below. This is due to quite a large amount of missing data. \n",
    "\n",
    "We will avoid a computationally efficient mean of finding the median due to complexity. \n",
    "\n",
    "A systematic but computationally inefficient approach to this would be to increase/decrease this median by a set amount until a balance is reached, or until we go past the median. If we go past the median, we step back and continue with a smaller interval. \n",
    "\n",
    "### Q3: Using this method, try to find the median by making changes and updating your guess. Start by using 1000, then 100, then 10, then 1 as your changes to the median. Write your \"solution path\" in the markdown box below. \n",
    "\n",
    "We will say an acceptable \"tolerance\" is 1. I.e. if you can say the median is between 100 and 101, that has a tolerance of 1. So we find the two consectutive integers where we would 'flip' in trying to find the median. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0\n"
     ]
    }
   ],
   "source": [
    "n_above = 0\n",
    "n_below = 0\n",
    "med_guess = ...\n",
    "\n",
    "print(n_below, n_above)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "..."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "dsc495-002_a2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
